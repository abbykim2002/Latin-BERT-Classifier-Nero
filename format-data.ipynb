{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fcd17d0",
   "metadata": {},
   "source": [
    "# Format Data\n",
    "\n",
    "This notebook splits the cleaned data (ie the three \"clean.txt\" files) into chunks of 10 words. It then uses LatinBERT to extract the sentence representation of each chunk and saves the features along with its classification in a dataframe that gets outputted to a csv file. The csv is formatted so the model can take data as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b3d2553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8381cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From gen_berts.py\n",
    "import argparse, sys\n",
    "from cltk.tokenizers.lat.lat import LatinWordTokenizer as WordTokenizer\n",
    "from cltk.tokenizers.lat.lat import LatinPunktSentenceTokenizer as SentenceTokenizer\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertPreTrainedModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LatinBERT():\n",
    "\n",
    "\tdef __init__(self, tokenizerPath=None, bertPath=None):\n",
    "\t\tencoder = text_encoder.SubwordTextEncoder(tokenizerPath)\n",
    "\t\tself.wp_tokenizer = LatinTokenizer(encoder)\n",
    "\t\tself.model = BertLatin(bertPath=bertPath)\n",
    "\t\tself.model.to(device)\n",
    "\n",
    "\tdef get_batches(self, sentences, max_batch, tokenizer):\n",
    "\n",
    "\t\t\tmaxLen=0\n",
    "\t\t\tfor sentence in sentences:\n",
    "\t\t\t\tlength=0\n",
    "\t\t\t\tfor word in sentence:\n",
    "\t\t\t\t\ttoks=tokenizer.tokenize(word)\n",
    "\t\t\t\t\tlength+=len(toks)\n",
    "\n",
    "\t\t\t\tif length> maxLen:\n",
    "\t\t\t\t\tmaxLen=length\n",
    "\n",
    "\t\t\tall_data=[]\n",
    "\t\t\tall_masks=[]\n",
    "\t\t\tall_labels=[]\n",
    "\t\t\tall_transforms=[]\n",
    "\n",
    "\t\t\tfor sentence in sentences:\n",
    "\t\t\t\ttok_ids=[]\n",
    "\t\t\t\tinput_mask=[]\n",
    "\t\t\t\tlabels=[]\n",
    "\t\t\t\ttransform=[]\n",
    "\n",
    "\t\t\t\tall_toks=[]\n",
    "\t\t\t\tn=0\n",
    "\t\t\t\tfor idx, word in enumerate(sentence):\n",
    "\t\t\t\t\ttoks=tokenizer.tokenize(word)\n",
    "\t\t\t\t\tall_toks.append(toks)\n",
    "\t\t\t\t\tn+=len(toks)\n",
    "\n",
    "\t\t\t\tcur=0\n",
    "\t\t\t\tfor idx, word in enumerate(sentence):\n",
    "\t\t\t\t\ttoks=all_toks[idx]\n",
    "\t\t\t\t\tind=list(np.zeros(n))\n",
    "\t\t\t\t\tfor j in range(cur,cur+len(toks)):\n",
    "\t\t\t\t\t\tind[j]=1./len(toks)\n",
    "\t\t\t\t\tcur+=len(toks)\n",
    "\t\t\t\t\ttransform.append(ind)\n",
    "\n",
    "\t\t\t\t\ttok_ids.extend(tokenizer.convert_tokens_to_ids(toks))\n",
    "\n",
    "\t\t\t\t\tinput_mask.extend(np.ones(len(toks)))\n",
    "\t\t\t\t\tlabels.append(1)\n",
    "\n",
    "\t\t\t\tall_data.append(tok_ids)\n",
    "\t\t\t\tall_masks.append(input_mask)\n",
    "\t\t\t\tall_labels.append(labels)\n",
    "\t\t\t\tall_transforms.append(transform)\n",
    "\n",
    "\t\t\tlengths = np.array([len(l) for l in all_data])\n",
    "\n",
    "\t\t\t# Note sequence must be ordered from shortest to longest so current_batch will work\n",
    "\t\t\tordering = np.argsort(lengths)\n",
    "\t\t\t\n",
    "\t\t\tordered_data = [None for i in range(len(all_data))]\n",
    "\t\t\tordered_masks = [None for i in range(len(all_data))]\n",
    "\t\t\tordered_labels = [None for i in range(len(all_data))]\n",
    "\t\t\tordered_transforms = [None for i in range(len(all_data))]\n",
    "\t\t\t\n",
    "\n",
    "\t\t\tfor i, ind in enumerate(ordering):\n",
    "\t\t\t\tordered_data[i] = all_data[ind]\n",
    "\t\t\t\tordered_masks[i] = all_masks[ind]\n",
    "\t\t\t\tordered_labels[i] = all_labels[ind]\n",
    "\t\t\t\tordered_transforms[i] = all_transforms[ind]\n",
    "\n",
    "\t\t\tbatched_data=[]\n",
    "\t\t\tbatched_mask=[]\n",
    "\t\t\tbatched_labels=[]\n",
    "\t\t\tbatched_transforms=[]\n",
    "\n",
    "\t\t\ti=0\n",
    "\t\t\tcurrent_batch=max_batch\n",
    "\n",
    "\t\t\twhile i < len(ordered_data):\n",
    "\n",
    "\t\t\t\tbatch_data=ordered_data[i:i+current_batch]\n",
    "\t\t\t\tbatch_mask=ordered_masks[i:i+current_batch]\n",
    "\t\t\t\tbatch_labels=ordered_labels[i:i+current_batch]\n",
    "\t\t\t\tbatch_transforms=ordered_transforms[i:i+current_batch]\n",
    "\n",
    "\t\t\t\tmax_len = max([len(sent) for sent in batch_data])\n",
    "\t\t\t\tmax_label = max([len(label) for label in batch_labels])\n",
    "\n",
    "\t\t\t\tfor j in range(len(batch_data)):\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tblen=len(batch_data[j])\n",
    "\t\t\t\t\tblab=len(batch_labels[j])\n",
    "\n",
    "\t\t\t\t\tfor k in range(blen, max_len):\n",
    "\t\t\t\t\t\tbatch_data[j].append(0)\n",
    "\t\t\t\t\t\tbatch_mask[j].append(0)\n",
    "\t\t\t\t\t\tfor z in range(len(batch_transforms[j])):\n",
    "\t\t\t\t\t\t\tbatch_transforms[j][z].append(0)\n",
    "\n",
    "\t\t\t\t\tfor k in range(blab, max_label):\n",
    "\t\t\t\t\t\tbatch_labels[j].append(-100)\n",
    "\n",
    "\t\t\t\t\tfor k in range(len(batch_transforms[j]), max_label):\n",
    "\t\t\t\t\t\tbatch_transforms[j].append(np.zeros(max_len))\n",
    "\n",
    "\t\t\t\tbatched_data.append(torch.LongTensor(batch_data))\n",
    "\t\t\t\tbatched_mask.append(torch.FloatTensor(batch_mask))\n",
    "\t\t\t\tbatched_labels.append(torch.LongTensor(batch_labels))\n",
    "\t\t\t\tbatched_transforms.append(torch.FloatTensor(batch_transforms))\n",
    "\n",
    "\t\t\t\tbsize=torch.FloatTensor(batch_transforms).shape\n",
    "\t\t\t\t\n",
    "\t\t\t\ti+=current_batch\n",
    "\n",
    "\t\t\t\t# adjust batch size; sentences are ordered from shortest to longest so decrease as they get longer\n",
    "\t\t\t\tif max_len > 100:\n",
    "\t\t\t\t\tcurrent_batch=12\n",
    "\t\t\t\tif max_len > 200:\n",
    "\t\t\t\t\tcurrent_batch=6\n",
    "\n",
    "\t\t\treturn batched_data, batched_mask, batched_transforms, ordering\n",
    "\n",
    "\n",
    "\tdef get_berts(self, raw_sents):\n",
    "\t\tsents=convert_to_toks(raw_sents)\n",
    "\t\tbatch_size=32\n",
    "\t\tbatched_data, batched_mask, batched_transforms, ordering=self.get_batches(sents, batch_size, self.wp_tokenizer)\n",
    "\n",
    "\t\tordered_preds=[]\n",
    "\t\tfor b in range(len(batched_data)):\n",
    "\t\t\tsize=batched_transforms[b].shape\n",
    "\t\t\tb_size=size[0]\n",
    "\t\t\tberts=self.model.forward(batched_data[b], attention_mask=batched_mask[b], transforms=batched_transforms[b])\n",
    "\t\t\tberts=berts.detach()\n",
    "\t\t\tberts=berts.cpu()\n",
    "\t\t\tfor row in range(b_size):\n",
    "\t\t\t\tordered_preds.append([np.array(r) for r in berts[row]])\n",
    "\n",
    "\t\tpreds_in_order = [None for i in range(len(sents))]\n",
    "\n",
    "\n",
    "\t\tfor i, ind in enumerate(ordering):\n",
    "\t\t\tpreds_in_order[ind] = ordered_preds[i]\n",
    "\n",
    "\n",
    "\t\tbert_sents=[]\n",
    "\n",
    "\t\tfor idx, sentence in enumerate(sents):\n",
    "\t\t\tbert_sent=[]\n",
    "\n",
    "\t\t\tbert_sent.append((\"[CLS]\", preds_in_order[idx][0] ))\n",
    "\n",
    "\t\t\tfor t_idx in range(1, len(sentence)-1):\n",
    "\t\t\t\ttoken=sentence[t_idx]\n",
    "\t\t\t\t\n",
    "\t\t\t\tpred=preds_in_order[idx][t_idx]\n",
    "\t\t\t\tbert_sent.append((token, pred ))\n",
    "\n",
    "\t\t\tbert_sent.append((\"[SEP]\", preds_in_order[idx][len(sentence)-1] ))\n",
    "\n",
    "\t\t\tbert_sents.append(bert_sent)\n",
    "\n",
    "\t\treturn bert_sents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LatinTokenizer():\n",
    "\tdef __init__(self, encoder):\n",
    "\t\tself.vocab={}\n",
    "\t\tself.reverseVocab={}\n",
    "\t\tself.encoder=encoder\n",
    "\n",
    "\t\tself.vocab[\"[PAD]\"]=0\n",
    "\t\tself.vocab[\"[UNK]\"]=1\n",
    "\t\tself.vocab[\"[CLS]\"]=2\n",
    "\t\tself.vocab[\"[SEP]\"]=3\n",
    "\t\tself.vocab[\"[MASK]\"]=4\n",
    "\n",
    "\t\tfor key in self.encoder._subtoken_string_to_id:\n",
    "\t\t\tself.vocab[key]=self.encoder._subtoken_string_to_id[key]+5\n",
    "\t\t\tself.reverseVocab[self.encoder._subtoken_string_to_id[key]+5]=key\n",
    "\n",
    "\n",
    "\tdef convert_tokens_to_ids(self, tokens):\n",
    "\t\twp_tokens=[]\n",
    "\t\tfor token in tokens:\n",
    "\t\t\tif token == \"[PAD]\":\n",
    "\t\t\t\twp_tokens.append(0)\n",
    "\t\t\telif token == \"[UNK]\":\n",
    "\t\t\t\twp_tokens.append(1)\n",
    "\t\t\telif token == \"[CLS]\":\n",
    "\t\t\t\twp_tokens.append(2)\n",
    "\t\t\telif token == \"[SEP]\":\n",
    "\t\t\t\twp_tokens.append(3)\n",
    "\t\t\telif token == \"[MASK]\":\n",
    "\t\t\t\twp_tokens.append(4)\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\twp_tokens.append(self.vocab[token])\n",
    "\n",
    "\t\treturn wp_tokens\n",
    "\n",
    "\tdef tokenize(self, text):\n",
    "\t\ttokens=text.split(\" \")\n",
    "\t\twp_tokens=[]\n",
    "\t\tfor token in tokens:\n",
    "\n",
    "\t\t\tif token in {\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"}:\n",
    "\t\t\t\twp_tokens.append(token)\n",
    "\t\t\telse:\n",
    "\n",
    "\t\t\t\twp_toks=self.encoder.encode(token)\n",
    "\n",
    "\t\t\t\tfor wp in wp_toks:\n",
    "\t\t\t\t\twp_tokens.append(self.reverseVocab[wp+5])\n",
    "\n",
    "\t\treturn wp_tokens\n",
    "\n",
    "def convert_to_toks(sents):\n",
    "\n",
    "\tsent_tokenizer = SentenceTokenizer()\n",
    "\tword_tokenizer = WordTokenizer()\n",
    "\n",
    "\tall_sents=[]\n",
    "\n",
    "\tfor data in sents:\n",
    "\t\ttext=data.lower()\n",
    "\n",
    "\t\tsents=sent_tokenizer.tokenize(text)\n",
    "\t\tfor sent in sents:\n",
    "\t\t\ttokens=word_tokenizer.tokenize(sent)\n",
    "\t\t\tfilt_toks=[]\n",
    "\t\t\tfilt_toks.append(\"[CLS]\")\n",
    "\t\t\tfor tok in tokens:\n",
    "\t\t\t\tif tok != \"\":\n",
    "\t\t\t\t\tfilt_toks.append(tok)\n",
    "\t\t\tfilt_toks.append(\"[SEP]\")\n",
    "\n",
    "\t\t\tall_sents.append(filt_toks)\n",
    "\n",
    "\treturn all_sents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BertLatin(nn.Module):\n",
    "\n",
    "\tdef __init__(self, bertPath=None):\n",
    "\t\tsuper(BertLatin, self).__init__()\n",
    "\n",
    "\t\tself.bert = BertModel.from_pretrained(bertPath)\n",
    "\t\tself.bert.eval()\n",
    "\t\t\n",
    "\tdef forward(self, input_ids, token_type_ids=None, attention_mask=None, transforms=None):\n",
    "\n",
    "\t\tinput_ids = input_ids.to(device)\n",
    "\t\tattention_mask = attention_mask.to(device)\n",
    "\t\ttransforms = transforms.to(device)\n",
    "\t\tsequence_outputs, pooled_outputs = self.bert.forward(input_ids, token_type_ids=None, attention_mask=attention_mask)\n",
    "\n",
    "\t\tall_layers=sequence_outputs\n",
    "\t\tout=torch.matmul(transforms,all_layers)\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d01081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates dataframe of latin sentences, if it is history or myth, and its features extracted from LatinBERT\n",
    "# sents = list of sentences, isHistory = 1/0\n",
    "def getDF(sents, isHistory):\n",
    "    bertPath=\"latin-bert-master/models/latin_bert/\"\n",
    "    tokenizerPath=\"latin-bert-master/models/subword_tokenizer_latin/latin.subword.encoder\"\n",
    "\n",
    "    bert=LatinBERT(tokenizerPath=tokenizerPath, bertPath=bertPath)\n",
    "    \n",
    "    bert_sents=bert.get_berts(sents)\n",
    "\n",
    "    rows = set([])\n",
    "\n",
    "    count = 0\n",
    "    for sent in bert_sents:\n",
    "        vector = np.array([])\n",
    "        for (token, bert) in sent[1:-1]:\n",
    "            vector = np.concatenate((vector, bert), axis=0)\n",
    "        temp = [str(sents[count]), int(isHistory)]+[float(i) for i in vector[:7680]] # tokenizer sometimes splits words, truncate as solution\n",
    "        rows.add(tuple(temp))\n",
    "        count += 1\n",
    "    return pd.DataFrame(rows, columns = ['latin', 'isHistory']+[i for i in range(7680)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b3cb0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69235"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load history data\n",
    "wordList = []\n",
    "file = 'data/history/clean.txt'\n",
    "\n",
    "with open(file, \"r\") as f:\n",
    "    wordList = f.read().split()\n",
    "\n",
    "sentences = set([])\n",
    "\n",
    "for i in range(0, len(wordList), 10):\n",
    "    sentences.add(' '.join(wordList[i:i+10]))\n",
    "\n",
    "# sentences is a set of strings where each string is 10 words of latin\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1523cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26000\n",
      "27000\n"
     ]
    }
   ],
   "source": [
    "# output features of history data as a series of dataframes --> csv files\n",
    "listSentences = list(sentences)\n",
    "\n",
    "for i in range(0, 60000, 3000):\n",
    "    getDF(listSentences[i:i+1000], 1).to_csv('history-dfs/'+str(i//1000)+'.csv')\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fccb1ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30464"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load myth data\n",
    "wordList = []\n",
    "file = 'data/myth/clean.txt'\n",
    "\n",
    "with open(file, \"r\") as f:\n",
    "    wordList = f.read().split()\n",
    "\n",
    "sentencesM = set([])\n",
    "\n",
    "for i in range(0, len(wordList), 10):\n",
    "    sentencesM.add(' '.join(wordList[i:i+10]))\n",
    "\n",
    "# sentences is a set of strings where each string is 10 words of latin\n",
    "len(sentencesM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bdf692c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rv/70f4qh790dx4d3n6r7_9bhs80000gn/T/ipykernel_40639/3253452958.py:125: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/pytorch/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  batched_transforms.append(torch.FloatTensor(batch_transforms))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "# output features of myth data as a series of dataframes --> csv files\n",
    "listSentencesM = list(sentencesM)\n",
    "\n",
    "#only get 10,000 data points\n",
    "for i in range(0, 21000, 2000):\n",
    "    getDF(listSentencesM[i:i+1000], 0).to_csv('myth-dfs/'+str(i//1000)+'.csv')\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34088128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2338"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load nero data\n",
    "wordList = []\n",
    "file = 'data/nero/clean.txt'\n",
    "\n",
    "with open(file, \"r\") as f:\n",
    "    wordList = f.read().split()\n",
    "\n",
    "sentences = set([])\n",
    "\n",
    "for i in range(0, len(wordList), 10):\n",
    "    sentences.add(' '.join(wordList[i:i+10]))\n",
    "\n",
    "# sentences is a set of strings where each string is 10 words of latin\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bd68826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "# output features of nero data as a series of dataframes --> csv files\n",
    "listSentencesN = list(sentences)\n",
    "\n",
    "for i in range(0, len(sentences), 1000):\n",
    "    getDF(listSentencesN[i:i+1000], 0).to_csv('nero-dfs/'+str(i//1000)+'.csv')\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aec78778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1573"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load nero - annals data\n",
    "wordList = []\n",
    "file = 'data/nero/clean-annals.txt'\n",
    "\n",
    "with open(file, \"r\") as f:\n",
    "    wordList = f.read().split()\n",
    "\n",
    "sentences = set([])\n",
    "\n",
    "for i in range(0, len(wordList), 10):\n",
    "    sentences.add(' '.join(wordList[i:i+10]))\n",
    "\n",
    "# sentences is a set of strings where each string is 10 words of latin\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82187069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rv/70f4qh790dx4d3n6r7_9bhs80000gn/T/ipykernel_43877/3253452958.py:125: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/pytorch/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  batched_transforms.append(torch.FloatTensor(batch_transforms))\n"
     ]
    }
   ],
   "source": [
    "# output features of nero-annals data as a dataframe --> csv\n",
    "listSentencesN = list(sentences)\n",
    "\n",
    "getDF(listSentencesN, 0).to_csv('nero-dfs/annals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fd1da0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load nero - bio data\n",
    "wordList = []\n",
    "file = 'data/nero/clean-biography.txt'\n",
    "\n",
    "with open(file, \"r\") as f:\n",
    "    wordList = f.read().split()\n",
    "\n",
    "sentences = set([])\n",
    "\n",
    "for i in range(0, len(wordList), 10):\n",
    "    sentences.add(' '.join(wordList[i:i+10]))\n",
    "\n",
    "# sentences is a set of strings where each string is 10 words of latin\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00b3f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output features of nero-annals data as a dataframe --> csv\n",
    "listSentencesN = list(sentences)\n",
    "\n",
    "getDF(listSentencesN, 0).to_csv('nero-dfs/biography.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189abbdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
